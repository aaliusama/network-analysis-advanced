{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bda05f",
   "metadata": {},
   "source": [
    "# Practical session 5: Core-periphery, community detection, motifs, and network filtering\n",
    "\n",
    "In this practical session, we will learn how to find communities in networks.\n",
    "\n",
    "We will use NetworkX version 3.5 (see [documentation](https://networkx.org/documentation/networkx-3.5/reference/index.html) and [tutorial](https://networkx.org/documentation/networkx-3.5/tutorial.html)).\n",
    "\n",
    "\n",
    "#### Credits\n",
    "This notebook is adapted from the tutorials associated to the book \"A FIRST COURSE IN NETWORK SCIENCE\" ([link to the GitHub repository](https://github.com/CambridgeUniversityPress/FirstCourseNetworkScience)). The original content is licensed under the [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a67de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import cpnet\n",
    "\n",
    "\n",
    "print(\"NetworkX version: \", nx.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85867ee",
   "metadata": {},
   "source": [
    "# Core-periphery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9686b0",
   "metadata": {},
   "source": [
    "Let's create a core-periphery network manually and apply the core-periphery detection algorithms.\n",
    "\n",
    "We will use the the \"Core\" module from NetworkX:\n",
    "\n",
    "https://networkx.org/documentation/networkx-3.5/reference/algorithms/core.html\n",
    "\n",
    "and the `cpnet` library:\n",
    "\n",
    "https://github.com/skojaku/core-periphery-detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e9a8a4",
   "metadata": {},
   "source": [
    "To create a core-periphery network, we proceed as follows:\n",
    "- choose the number of core nodes `n_core` and the number of periphery nodes `n_periphery`\n",
    "- create a Erdos-Renyi random graph among the core nodes with a relatively high connection probability `p_core`\n",
    "- create a Erdos-Renyi random graph among the periphery nodes with a relatively low connection probability `p_periphery`\n",
    "- create edges between core and periphery nodes with a intermediate connection probability `p_core_periphery`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da891357",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG = np.random.RandomState(42)\n",
    "\n",
    "# number of core and peripheral nodes\n",
    "n_core = 20\n",
    "n_periphery = 50\n",
    "\n",
    "# create the core\n",
    "p_core = 0.5\n",
    "G_core = nx.erdos_renyi_graph(n_core, p_core)\n",
    "\n",
    "# create the periphery\n",
    "p_periphery = 0.05\n",
    "G_per = nx.erdos_renyi_graph(n_periphery, p_periphery)\n",
    "\n",
    "# relabel periphery nodes such that their names go from n_core to n_core + n_periphery - 1\n",
    "# and core nodes are from 0 to n_core - 1\n",
    "G_per = nx.relabel_nodes(G_per, lambda x: x + n_core)\n",
    "\n",
    "# build the union of the two graphs\n",
    "G = nx.union(G_core, G_per)\n",
    "\n",
    "all_nodes = list(G.nodes())\n",
    "core_nodes = list(range(n_core))\n",
    "periphery_nodes = list(range(n_core, n_core + n_periphery))\n",
    "\n",
    "# create edges between core and periphery\n",
    "p_core_periphery = 0.15\n",
    "for node in periphery_nodes:\n",
    "    for core_node in core_nodes:\n",
    "        if RNG.rand() < p_core_periphery:\n",
    "            G.add_edge(node, core_node)\n",
    "\n",
    "print(\"Network has\", G.number_of_nodes(), \"nodes and\", G.number_of_edges(), \"edges\")\n",
    "\n",
    "# draw\n",
    "colors = [\"red\"] * n_core + [\"skyblue\"] * n_periphery\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "nx.draw(G, pos, node_color=colors, with_labels=False, node_size=120, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bc7044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot adjacency matrix\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(nx.adjacency_matrix(G).todense(), ax=ax, cmap=\"Blues\", cbar=False)\n",
    "\n",
    "ax.axvline(n_core, color=\"red\", linestyle=\"--\")\n",
    "ax.axhline(n_core, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax.set_xlabel(\"Node index\")\n",
    "ax.set_ylabel(\"Node index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9c88a",
   "metadata": {},
   "source": [
    "### K-core decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf069b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-core decomposition\n",
    "# dict: node -> core number\n",
    "core_numbers = nx.core_number(G)\n",
    "\n",
    "print(\"Core numbers:\", core_numbers.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac60b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 3))\n",
    "\n",
    "# count how many times each core number appears\n",
    "n_nodes_in_cores = Counter(core_numbers.values())\n",
    "\n",
    "max_core_num = max(n_nodes_in_cores.keys())\n",
    "n_nodes_in_cores = {k: n_nodes_in_cores.get(k, 0) for k in range(1, max_core_num + 1)}\n",
    "\n",
    "ax.bar(n_nodes_in_cores.keys(), n_nodes_in_cores.values())\n",
    "\n",
    "ax.set_xticks(range(1, max_core_num + 1))\n",
    "ax.set_xlabel(\"Core number\")\n",
    "ax.set_ylabel(\"Number of nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f0cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize according to k-core shell\n",
    "cmap = plt.cm.viridis\n",
    "norm = plt.Normalize(vmin=min(core_numbers.values()), vmax=max(core_numbers.values()))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "nx.draw(G, pos, node_color=[cmap(norm(core_numbers[n])) for n in G.nodes()],\n",
    "        with_labels=False, node_size=120, ax=ax)\n",
    "\n",
    "ax.set_title(\"k-core Decomposition\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5eb20",
   "metadata": {},
   "source": [
    "### Borgatti and Everett's method\n",
    "\n",
    "The `cpnet` library implements a wide variety of core-periphery detection algorithms, including the original method by Borgatti and Everett (2000).\n",
    "By running this method, we can obtain which nodes belong to the core and which to the periphery.\n",
    "\n",
    "The package works as follows:\n",
    "```python\n",
    "algorithm = cpnet.BE(G)\n",
    "node_assignment = algorithm.get_pair_id()\n",
    "node_coreness = algorithm.get_coreness()\n",
    "```\n",
    "\n",
    "`node_assignment` and `node_coreness` are python dict objects that take node labels (i.e., G.nodes()) as keys.\n",
    "- The values of node_assignment are integers indicating group ids: nodes having the same integer belong to the same group.\n",
    "- The values of node_coreness are float values indicating coreness ranging between 0 and 1. A larger value indicates that the node is closer to the core. In case of discrete core-periphery structure, the corenss can only take 0 or 1, with node_coreness[i]=1 or =0 indicating that node i belongs to a core or a periphery, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: cpnet is not deterministic and seed cannot be set\n",
    "niters = 50\n",
    "\n",
    "max_score = None\n",
    "best_coreperiphery = None\n",
    "for _ in range(niters):\n",
    "\n",
    "    be_cp = cpnet.BE(num_runs=10)\n",
    "    be_cp.detect(G)\n",
    "\n",
    "    # get the core-periphery assignment of nodes\n",
    "    node_coreness = be_cp.get_coreness()\n",
    "    node_assignment = be_cp.get_pair_id()\n",
    "\n",
    "    # compute the loss score\n",
    "    score = be_cp.score(G, node_assignment, node_coreness)\n",
    "\n",
    "    if max_score is None or score > max_score:\n",
    "        max_score = score\n",
    "        best_coreperiphery = be_cp\n",
    "\n",
    "print(\"Best core–periphery score:\", max_score)\n",
    "\n",
    "# now extract core and periphery nodes\n",
    "node_coreness = best_coreperiphery.get_coreness()\n",
    "node_assignment = best_coreperiphery.get_pair_id()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05500ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the assignment of coreness of each node\n",
    "x = np.array([node_coreness[node] for node in G.nodes()])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101efddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the C_ij matrix, multiply x_i with x_j\n",
    "C_ij = np.zeros((G.number_of_nodes(), G.number_of_nodes()))\n",
    "for i in range(G.number_of_nodes()):\n",
    "    for j in range(G.number_of_nodes()):\n",
    "        C_ij[i, j] = x[i] * x[j]\n",
    "\n",
    "C_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6117c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the fitted core–periphery structure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(C_ij, cmap=\"Blues\", cbar=False, ax=ax)\n",
    "\n",
    "ax.axvline(n_core, color=\"red\", linestyle=\"--\")\n",
    "ax.axhline(n_core, color=\"red\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52094ae0",
   "metadata": {},
   "source": [
    "# Community detection\n",
    "\n",
    "### Partitions\n",
    "\n",
    "A **partition** of a graph is a separation of its nodes into disjoint groups. Consider the following graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "\n",
    "G_toy = nx.Graph()\n",
    "nx.add_cycle(G_toy, [0, 1, 2, 3])\n",
    "nx.add_cycle(G_toy, [4, 5, 6, 7])\n",
    "G_toy.add_edge(0, 7)\n",
    "\n",
    "pos_toy = nx.spring_layout(G_toy, seed=42)\n",
    "nx.draw(G_toy, pos=pos_toy, with_labels=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc6743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of partition\n",
    "partition = [\n",
    "    {1, 2, 3},\n",
    "    {4, 5, 6},\n",
    "    {0, 7},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b28f4",
   "metadata": {},
   "source": [
    "Observe that every node in the graph is in exactly one of the sets in the partition. Formally, a partition is a list of sets such that every node is in exactly one set. NetworkX can verify that our partition is valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efbf8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can check using networkX functions\n",
    "nx.community.is_partition(G_toy, partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68122c53",
   "metadata": {},
   "source": [
    "When developing community detection algorithms, we often make use of a *partition map*, which is a dictionary mapping node names to a partition index. This is useful for quickly comparing if two nodes are in the same cluster in the partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e441404",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_map = {}\n",
    "for idx, cluster_nodes in enumerate(partition):\n",
    "    for node in cluster_nodes:\n",
    "        partition_map[node] = idx\n",
    "\n",
    "partition_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c59c83",
   "metadata": {},
   "source": [
    "Nodes with same partition index belong to the same community.\n",
    "\n",
    "We can visualize our partition by drawing the graph with nodes colored by their partition membership:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "\n",
    "node_colors = [partition_map[n] for n in G_toy.nodes]\n",
    "        \n",
    "nx.draw(G_toy, pos=pos_toy, node_color=node_colors, with_labels=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9fc2ee",
   "metadata": {},
   "source": [
    "Given that there is no unanimously accepted mathematical definition of community, we would like quantitative methods to partition nodes in a meaningful way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a969c",
   "metadata": {},
   "source": [
    "## Community detection algorithms: Girvan-Newman method\n",
    "\n",
    "It is an iterative algorithm whose core idea consists of, roughly, removing edges linking highly connected regions of the network. We can quantify this property using the edge betweenness centrality (similar to the node betweenness but computed on edges).\n",
    "\n",
    "The algorithm is:\n",
    "1. compute the edge betweenness centrality of all edges,\n",
    "2. remove the edge with the largest centrality value (in case of ties, one of them at random),\n",
    "3. assign one community to each connected component,\n",
    "4. repeat steps 1-3 until all nodes are isolated\n",
    "\n",
    "This is an example of an divisive community detection method: we start from all nodes belonging to the same commuity and we proceed until all nodes belong to different communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6758eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def girvan_newman_communities(G, max_n_communities=None):\n",
    "\n",
    "    # make a copy of the network\n",
    "    G_copy = G.copy()\n",
    "\n",
    "    # count number of connected components\n",
    "    curr_n_conn_components = nx.number_connected_components(G_copy)\n",
    "    \n",
    "    # store communities found at each step\n",
    "    # eg. [[{1,2,3}, {4, 5}], [{1}, {2, 3}, {4, 5}], ...]\n",
    "    communities = []\n",
    "    pbar = tqdm(total=G_copy.number_of_edges(), leave=False)\n",
    "    while G_copy.number_of_edges() > 0:\n",
    "\n",
    "        # compute edge betweenness and remove edge with highest betweenness\n",
    "        edge_betweenness = nx.edge_betweenness_centrality(G_copy)\n",
    "        edge_betweenness = sorted(edge_betweenness.items(), key=lambda x: x[1], reverse=True)\n",
    "        edge_max_betweenness = edge_betweenness[0][0]\n",
    "\n",
    "        G_copy.remove_edge(*edge_max_betweenness)\n",
    "        pbar.update(1)\n",
    "\n",
    "        # get the connected components\n",
    "        conn_components = list(nx.connected_components(G_copy))\n",
    "        n_conn_comp = len(conn_components)\n",
    "\n",
    "        if n_conn_comp > curr_n_conn_components:\n",
    "            # new communities are formed\n",
    "            communities.append(conn_components)\n",
    "            curr_n_conn_components = n_conn_comp\n",
    "\n",
    "            if max_n_communities is not None and curr_n_conn_components >= max_n_communities:\n",
    "                break\n",
    "\n",
    "    # leave the progress bar\n",
    "    pbar.close()\n",
    "\n",
    "    return communities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff567fd3",
   "metadata": {},
   "source": [
    "## Application to a stochastic block model network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ae463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's generate a planted partition with a stochastic block model\n",
    "RNG = np.random.RandomState(42)\n",
    "\n",
    "# number of nodes in each block\n",
    "block_sizes = [20, 50, 30, 20]\n",
    "\n",
    "# connection probabilities between and within blocks\n",
    "# within blocks given by the diagonal entries\n",
    "# between blocks by the off-diagonal entries\n",
    "blocks_p = [\n",
    "    [0.60, 0.30, 0.01, 0.01],\n",
    "    [0.30, 0.50, 0.01, 0.01],\n",
    "    [0.01, 0.01, 0.40, 0.01],\n",
    "    [0.01, 0.01, 0.01, 0.50],\n",
    "]\n",
    "\n",
    "G = nx.stochastic_block_model(block_sizes, p=blocks_p,  seed=RNG, directed=False, selfloops=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a637115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the block connection probabilities\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "\n",
    "sns.heatmap(blocks_p, vmin=0, vmax=0.5, cmap=\"Blues\", cbar=True, ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"Block index\")\n",
    "ax.set_ylabel(\"Block index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68880784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the network\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "node_colors = [attr['block'] for _, attr in G.nodes(data=True)]\n",
    "\n",
    "nx.draw(G, pos=pos, node_size=50, node_color=node_colors, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86089a90",
   "metadata": {},
   "source": [
    "How many communities do you expect to find?\n",
    "\n",
    "Let's apply the Girvan-Newman method to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3617d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on our implementation, it returns all the community splits found\n",
    "communities_gn_list = girvan_newman_communities(G, max_n_communities=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6cc166",
   "metadata": {},
   "source": [
    "Let's visualize one of such partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4be58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "# make the partition map\n",
    "choosen_gn_partition = communities_gn_list[4]\n",
    "\n",
    "print(\"Number of communities in the chosen partition:\", len(choosen_gn_partition))\n",
    "\n",
    "partition_map_1 = {}\n",
    "for idx, cluster_nodes in enumerate(choosen_gn_partition):\n",
    "    for node in cluster_nodes:\n",
    "        partition_map_1[node] = idx\n",
    "\n",
    "# plot the network and compute modularity\n",
    "node_colors = [partition_map_1[n] for n in G.nodes]  \n",
    "nx.draw(G, pos=pos, node_color=node_colors, node_size=50, ax=ax)\n",
    "\n",
    "mod_1 = nx.community.modularity(G, choosen_gn_partition)\n",
    "ax.set_title(f\"Q = {mod_1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f1fa9",
   "metadata": {},
   "source": [
    "How can we evaluate the quality of a partition?\n",
    "\n",
    "One of the first proposals is the *modularity* (introduced in 2004):\n",
    "$$ Q = \\frac{1}{2|E|} \\sum_{i,j\\ne i}^N \\left(A_{i,j} - \\frac{k_i k_j}{2|E|} \\right) \\delta\\left(C_i, C_j \\right) $$\n",
    "\n",
    "The modularity of a graph partition compares the number of intra-group edges with a random baseline (i.e., a configuration model). Higher modularity scores correspond to a higher proportion of intra-group edges, therefore fewer inter-group edges and better separation of groups.\n",
    "\n",
    "NetworkX has a function to compute the modularity of a partition: `nx.community.modularity` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a6840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the modularity of the obtained partitions\n",
    "Qs_gn = [nx.algorithms.community.quality.modularity(G, comm) for comm in communities_gn_list]\n",
    "n_communities_gn_list = [len(comm) for comm in communities_gn_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ae7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "ax.plot(n_communities_gn_list, Qs_gn, marker='o')\n",
    "\n",
    "# highlight the maximum\n",
    "idx_max = np.argmax(Qs_gn)\n",
    "n_communities_opt = n_communities_gn_list[idx_max]\n",
    "\n",
    "ax.plot(n_communities_opt, Qs_gn[idx_max], marker='o', color='red', markersize=10)\n",
    "\n",
    "ax.set_xlabel(\"Number of communities\")\n",
    "ax.set_ylabel(\"Modularity Q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ff1b1",
   "metadata": {},
   "source": [
    "Let's now compare the similarity between the optimal partition and the partition from which the network was generated.\n",
    "\n",
    "One possible measure is the Jaccard index:\n",
    "$$ J = \\frac{a_{11}}{a_{01} + a_{10} + a_{11}} $$\n",
    "\n",
    "where: \n",
    "- $a_{11}$ is the number of pairs of nodes that are in the same community in both partitions,\n",
    "- $a_{01}$ is the number of pairs of nodes that are in the same community in the first partition but not in the second,\n",
    "- $a_{10}$ is the number of pairs of nodes that are in the same community in the second partition but not in the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8cf3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_partition(part):\n",
    "    pairs_of_same_comm_nodes = set()\n",
    "    for comm in part:\n",
    "        for pair in combinations(comm, 2):\n",
    "            pair = tuple(sorted(pair))\n",
    "            pairs_of_same_comm_nodes.add(pair)\n",
    "\n",
    "    return pairs_of_same_comm_nodes\n",
    "\n",
    "\n",
    "def jaccard_index(part1, part2):\n",
    "\n",
    "    part1_flat = flatten_partition(part1)\n",
    "    part2_flat = flatten_partition(part2)\n",
    "\n",
    "    inters = len(set(part1_flat).intersection(set(part2_flat)))\n",
    "    union = len(set(part1_flat).union(set(part2_flat)))\n",
    "\n",
    "    return inters / union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3802d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the true partition\n",
    "part_true = [list(range(sum(block_sizes[:i]), sum(block_sizes[:i+1]))) for i in range(len(block_sizes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf321d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max = np.argmax(Qs_gn)\n",
    "best_partition_gn = communities_gn_list[idx_max]\n",
    "\n",
    "jaccard_index(part_true, best_partition_gn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106d2cbd",
   "metadata": {},
   "source": [
    "# Disparity filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e6f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ecological food web network seen in class\n",
    "# a directed link from i to j means that species i is preyed upon by species j\n",
    "G = nx.read_weighted_edgelist(\"../datasets/FloridaFoodWeb/edges.csv\", create_using=nx.DiGraph, delimiter=\",\")\n",
    "\n",
    "# get largest weakly connected component\n",
    "largest_wcc = max(nx.weakly_connected_components(G), key=len)\n",
    "G = G.subgraph(largest_wcc).copy()\n",
    "\n",
    "# remove self-loops\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073bac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define disparity filter method\n",
    "def compute_disparity_filter_probas(G):\n",
    "\n",
    "    # create a edge attribute that corresponds to the p_ij\n",
    "    edge_probas = {}\n",
    "    for node in G.nodes():\n",
    "\n",
    "        # triplets (i, j, w)\n",
    "        node_edges = G.edges(node, data='weight')\n",
    "        k = len(node_edges)\n",
    "\n",
    "        # compute the strength\n",
    "        strength = sum([w for _, _, w in node_edges])\n",
    "\n",
    "        for u, v, w in node_edges:\n",
    "            p_ij = (1 - w / strength) ** (k - 1)\n",
    "            edge_probas[(u, v)] = p_ij\n",
    "\n",
    "    if not G.is_directed():\n",
    "        # in general p_ij != p_ji\n",
    "        # for this reason, in undirected networks, we consider that an edge is significant if either p_ij or p_ji is smaller than alpha\n",
    "        # thus, we impose that p_ij = min(p_ij, p_ji)\n",
    "        for (u, v), p_ij in edge_probas.items():\n",
    "\n",
    "            if G.has_edge(v, u):\n",
    "                p_ij_min = min(edge_probas[(u, v)], edge_probas[(v, u)])\n",
    "                edge_probas[(u, v)] = p_ij_min\n",
    "                edge_probas[(v, u)] = p_ij_min\n",
    "\n",
    "    return edge_probas\n",
    "\n",
    "# set the new edge attribute\n",
    "edge_probas = compute_disparity_filter_probas(G)\n",
    "nx.set_edge_attributes(G, edge_probas, name='p_ij')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace06dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disparity_filter(G, p_ij_attr, alpha):\n",
    "    G_filtered = G.copy()\n",
    "    for edge in G.edges():\n",
    "        if G.edges[edge][p_ij_attr] >= alpha:\n",
    "            G_filtered.remove_edge(edge[0], edge[1])\n",
    "\n",
    "    # remove isolated nodes\n",
    "    isolated_nodes = list(nx.isolates(G_filtered))\n",
    "    G_filtered.remove_nodes_from(isolated_nodes)\n",
    "    \n",
    "    return G_filtered\n",
    "\n",
    "\n",
    "# define thresholding function\n",
    "def threshold_filter(G, threshold):\n",
    "    G_filtered = G.copy()\n",
    "    for edge in G.edges():\n",
    "        if G.edges[edge]['weight'] <= threshold:\n",
    "            G_filtered.remove_edge(edge[0], edge[1])\n",
    "\n",
    "    # remove isolated nodes\n",
    "    isolated_nodes = list(nx.isolates(G_filtered))\n",
    "    G_filtered.remove_nodes_from(isolated_nodes)\n",
    "    \n",
    "    return G_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the naive threshold filter using different values the the weight as a threshold\n",
    "all_edge_weights = [w for i, j, w in G.edges(data='weight')]\n",
    "min_threshold = min(all_edge_weights)\n",
    "max_threshold = max(all_edge_weights)\n",
    "\n",
    "thresholds = np.logspace(np.log10(min_threshold), np.log10(max_threshold), 500)\n",
    "\n",
    "results_threshold_df = []\n",
    "for threshold in tqdm(thresholds):\n",
    "\n",
    "    G_filtered = threshold_filter(G, threshold)\n",
    "\n",
    "    # get the number of nodes and edges of the filtered graph\n",
    "    num_nodes_retained = G_filtered.number_of_nodes()\n",
    "    num_edges_retained = G_filtered.number_of_edges()\n",
    "    \n",
    "    results_threshold_df.append([threshold, num_nodes_retained, num_edges_retained])\n",
    "\n",
    "results_threshold_df = pd.DataFrame(results_threshold_df, columns=['threshold', 'num_nodes', 'num_edges'])\n",
    "results_threshold_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67efe972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the disparity filter using different values of alpha\n",
    "alphas = np.logspace(-6, 0, 100)\n",
    "\n",
    "results_disparity_df = []\n",
    "for alpha in tqdm(alphas):\n",
    "    \n",
    "    G_filtered = disparity_filter(G, 'p_ij', alpha)\n",
    "\n",
    "    # remove isolated nodes\n",
    "    isolated_nodes = list(nx.isolates(G_filtered))\n",
    "    G_filtered.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "    # get the number of nodes and edges of the filtered graph\n",
    "    num_nodes_retained = G_filtered.number_of_nodes()\n",
    "    num_edges_retained = G_filtered.number_of_edges()\n",
    "\n",
    "    results_disparity_df.append([alpha, num_nodes_retained, num_edges_retained])\n",
    "\n",
    "results_disparity_df = pd.DataFrame(results_disparity_df, columns=['alpha', 'num_nodes', 'num_edges'])\n",
    "results_disparity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f4cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the fraction of retained nodes and edges\n",
    "N, M = G.number_of_nodes(), G.number_of_edges()\n",
    "\n",
    "results_threshold_df.loc[:, 'frac_retained_nodes'] = results_threshold_df['num_nodes'] / N\n",
    "results_threshold_df.loc[:, 'frac_retained_edges'] = results_threshold_df['num_edges'] / M\n",
    "\n",
    "results_disparity_df.loc[:, 'frac_retained_nodes'] = results_disparity_df['num_nodes'] / N\n",
    "results_disparity_df.loc[:, 'frac_retained_edges'] = results_disparity_df['num_edges'] / M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22187906",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "\n",
    "ax.plot(results_threshold_df['frac_retained_edges'], results_threshold_df['frac_retained_nodes'], \"o-\", label='Thresholding')\n",
    "ax.plot(results_disparity_df['frac_retained_edges'], results_disparity_df['frac_retained_nodes'], \"o-\", label='Disparity Filter')\n",
    "\n",
    "ax.set_xlabel('Fraction of retained edges')\n",
    "ax.set_ylabel('Fraction of retained nodes')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cea812",
   "metadata": {},
   "source": [
    "Let's look at the graph from right (i.e., the original graph) to left (graph completely deleted). The disparity filter is able retain all the nodes while removing most part of edges (>80%). In comparison, the threshold filter removes ~40% of the nodes to achieve a similar reduction of number of edges. In conclusion, by looking at the \"significant\" connection, we can reduce the number of edges while retaining a larger fraction of nodes. For the analysis of this system, the disparity filter would be preferable because it retains almost all elements of the system (i.e., nodes) while removing non-significant connections between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b5b1d",
   "metadata": {},
   "source": [
    "# Motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb20b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Florida food web dataset\n",
    "G = nx.read_weighted_edgelist(\"../datasets/FloridaFoodWeb/edges.csv\", create_using=nx.DiGraph, delimiter=\",\")\n",
    "\n",
    "# get largest weakly connected component\n",
    "largest_wcc = max(nx.weakly_connected_components(G), key=len)\n",
    "G = G.subgraph(largest_wcc).copy()\n",
    "\n",
    "# remove self-loops\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of all possible triads\n",
    "# see explanation of triad names in: https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.triads.triads_by_type.html#networkx.algorithms.triads.triads_by_type\n",
    "nx.triadic_census(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22174a7",
   "metadata": {},
   "source": [
    "However, are those three-node motifs occurring just by chance? Or there is a biological reason for their presence?\n",
    "\n",
    "To answer this question, we can compare the motif counts with those obtained from a null model. A common choice is the configuration model, which preserves the in-degree and out-degree of each node while randomizing the connections.\n",
    "\n",
    "Once we have generated multiple instances of the configuration model, we can compute the motif counts in each instance and compare them with the original graph using the z-score:\n",
    "$$ z = \\frac{N_{real} - \\mu_{null}}{\\sigma_{null}} $$\n",
    "where $N_{real}$ is the motif count in the real graph, and $\\mu_{null}$ and $\\sigma_{null}$ are the mean and standard deviation of the motif counts in the null model instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4b2665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the occurrence of each triad in random graphs generated by the directed configuration model\n",
    "# repeat 50 times\n",
    "RNG = np.random.RandomState(42)\n",
    "\n",
    "niters = 50\n",
    "triad_census_rand_df = []\n",
    "for _ in tqdm(range(niters), total=niters, leave=False):\n",
    "\n",
    "    G_rand = nx.directed_configuration_model(\n",
    "        [d for n, d in G.in_degree()],\n",
    "        [d for n, d in G.out_degree()],\n",
    "        create_using=nx.DiGraph, \n",
    "        seed=RNG)\n",
    "    \n",
    "    triad_census_rand_ = nx.triadic_census(G_rand)\n",
    "    triad_census_rand_df.append(triad_census_rand_)\n",
    "\n",
    "# append also the real one - column niters is the one containing the real counts\n",
    "triad_census_real = nx.triadic_census(G)\n",
    "triad_census_rand_df.append(triad_census_real)\n",
    "\n",
    "triad_census_rand_df = pd.DataFrame(triad_census_rand_df).T\n",
    "triad_census_rand_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the z-scores\n",
    "\n",
    "# compute mean and std occurrence of each motif in the random graphs (columns from 0 to niters-1)\n",
    "triad_census_rand_df.loc[:, \"avg\"] = triad_census_rand_df[0:niters].mean(axis=1)\n",
    "triad_census_rand_df.loc[:, \"std\"] = triad_census_rand_df[0:niters].std(axis=1)\n",
    "\n",
    "# compute the z-score - the niters column contains the real counts\n",
    "triad_census_rand_df.loc[:, \"z-score\"] = (triad_census_rand_df[niters] - triad_census_rand_df[\"avg\"]) / triad_census_rand_df[\"std\"]\n",
    "triad_census_rand_df.sort_values(\"z-score\", inplace=True)\n",
    "\n",
    "# take only relevant columns\n",
    "triad_census_rand_df = triad_census_rand_df[[\"avg\", \"std\", \"z-score\"]]\n",
    "\n",
    "triad_census_rand_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the motif profiles\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 3))\n",
    "\n",
    "ax.plot(triad_census_rand_df[\"z-score\"], \"o\")\n",
    "\n",
    "ax.axhline(0, color=\"gray\", linestyle=\"--\")\n",
    "\n",
    "ax.set_xlabel(\"Triad types\")\n",
    "ax.set_ylabel(\"z-score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c4c1b0",
   "metadata": {},
   "source": [
    "003 : 1, 2, 3\n",
    "\n",
    "030C : 1 <- 2 <- 3, 1 -> 3\n",
    "\n",
    "111U : 1 <-> 2 -> 3\n",
    "\n",
    "300 : 1 <-> 2 <-> 3, 1 <-> 3\n",
    "\n",
    "021D : 1 <- 2 -> 3\n",
    "\n",
    "030T : 1 -> 2 -> 3, 1 -> 3\n",
    "\n",
    "021U : 1 -> 2 <- 3\n",
    "\n",
    "What is your interpretation given the nature of the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9886bff1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netsci-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
